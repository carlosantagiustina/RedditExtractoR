{
    "contents" : "#' Returns relevant reddit URLs\n#' \n#' @examples\n#' \\dontrun{\n#' example_urls = reddit_urls(search_terms=\"science\")\n#' }\n#' \n#' @param search_terms A character string to be searched on Reddit.\n#' @param regex_filter An optional regular expression filter that will remove URLs with titles that do not match the condition.\n#' @param subreddit An optional character string that will restrict the search to the specified subreddits (separated by space).\n#' @param cn_threshold Comment number threshold that remove URLs with fewer comments that cn_threshold. 0 by default.\n#' @param page_threshold Page threshold that controls the number of pages is going to be searched for a given search word. 1 by default.\n#' @param sort_by Sorting parameter, either \"comments\" (default) or \"new\".\n#' @param wait_time wait time in seconds between page requests. 2 by default and it is also the minimum (API rate limit).\n#' @return A data frame with URLs (links), number of comments (num_comments), title (title),date (date) and subreddit (subreddit).\n#' @export\n\nreddit_urls = function(search_terms=NA,\n                       regex_filter=\"\",\n                       subreddit=NA,\n                       cn_threshold=0,\n                       page_threshold=1,\n                       sort_by=\"relevance\",\n                       wait_time=2){\n  \n  if(!grepl(\"^comments$|^new$|^relevance$\",sort_by)){stop(\"sort_by must be either 'new', 'comments' or 'relevance'\")}\n  \n  sterms       = ifelse(is.na(search_terms),NA,gsub(\"\\\\s\",\"+\",search_terms))\n                        \n  cached_links = data.frame(date = as.Date(character()), \n                            num_comments = numeric(), \n                            title = character(), \n                            subreddit=character(),\n                            URL = character())\n  \n  subreddit      = ifelse(is.na(subreddit),\"\",paste0(\"r/\",gsub(\"\\\\s+\",\"+\",subreddit),\"/\"))\n  sterms         = ifelse(is.na(sterms),\"\",paste0(\"q=\",sterms,\"&restrict_sr=on&\"))\n  sterms_prefix  = ifelse(sterms==\"\",\"new\",\"search\")\n  \n  search_address = search_query = paste0(\"https://www.reddit.com/\",subreddit,sterms_prefix,\".json?\",sterms,\"sort=\",sort_by)\n\n  next_page      = index = \"\"\n  page_counter   = 0\n  comm_filter    = 10000\n  \n  while(is.null(next_page)==FALSE & page_counter < page_threshold & comm_filter >= cn_threshold & length(index)>0){\n    \n    search_JSON  = tryCatch(RJSONIO::fromJSON(readLines(search_query, warn = FALSE)), error = function(e) NULL)\n    \n    if(is.null(search_JSON)){\n      cat(paste(\"Cannot connect to the website, skipping...\\n\"))\n      next\n      }\n    \n    else{\n      \n      contents      = search_JSON[[2]]$children\n\n      search_permalink    = paste0(\"http://www.reddit.com\",sapply(seq(contents),function(x)contents[[x]]$data$permalink))\n      search_num_comments = sapply(seq(contents),function(x)contents[[x]]$data$num_comments)\n      search_title        = sapply(seq(contents),function(x)contents[[x]]$data$title)\n      search_score        = sapply(seq(contents),function(x)contents[[x]]$data$score)\n      search_subreddit    = sapply(seq(contents),function(x)contents[[x]]$data$subreddit)\n      \n      index = which(search_num_comments >= cn_threshold & grepl(regex_filter,search_title,ignore.case=T,perl=T))\n      \n      if(length(index)>0){\n        \n        search_date  = format(as.Date(as.POSIXct(unlist(lapply(seq(contents),function(x)contents[[x]]$data$created_utc)),origin=\"1970-01-01\")),\"%d-%m-%y\")\n        \n        \n        temp_dat     = data.frame(date             = search_date,\n                                  num_comments     = search_num_comments, \n                                  title            = search_title, \n                                  subreddit        = search_subreddit,\n                                  URL              = search_permalink,\n                                  stringsAsFactors = FALSE\n        )[index,]\n        \n        cached_links = as.data.frame(rbind(cached_links,temp_dat))\n        \n        next_page    = search_JSON$data$after\n        comm_filter  = utils::tail(search_num_comments,1)\n        search_query = paste0(search_address,\"&after=\",next_page)\n        page_counter = page_counter + 1\n        \n      }\n      Sys.sleep(min(2,wait_time))\n    }\n  }\n  \n  final_table = cached_links[!duplicated(cached_links),]\n  \n  if(dim(final_table)[1]==0){\n    cat(paste(\"\\nNo results retrieved, check your query\"))} else{\n      \n      remove_row  = which(final_table[,1]==\"\")\n      \n      if(length(remove_row)>0){final_table = final_table[-remove_row,]}\n      \n      return(final_table)\n      \n    }\n}\n\n\n\n#' Extract data attributes\n#' \n#' @examples\n#' \\dontrun{\n#' example_attr = reddit_content(URL=\"reddit.com/r/gifs/comments/39tzsy/whale_watching\")\n#' }\n#' \n#' @param URL a string or a vector of strings with the URL address of pages of interest\n#' @param wait_time wait time in seconds between page requests. 2 by default and it is also the minimum (API rate limit).\n#' @return A data frame with structure / position of the comment with respect to other comments (structure), ID (id), post / thread date (post_date), \n#' comment date (comm_date), number of comments within a post / thread (num_comments), subreddit (subreddit)\n#' upvote proportion (upvote_prop), post /thread score (post_score), author of the post / thread (author), user corresponding to the comment (user),\n#' comment score (comment_score), controversiality (controversiality), comment (comment), title (title), post / thread text (post_text), URL referenced (link)\n#' domain of the references URL (domain)\n#' @export\n\nreddit_content = function(URL,wait_time=2){\n  \n  if(is.null(URL) | length(URL)==0 | !is.character(URL)){stop(\"invalid URL parameter\")}\n  \n  # setting up a function for extraction of comment specific information:\n  GetAttribute  = function(node,feature){\n    Attribute   = node$data[[feature]]\n    replies     = node$data$replies\n    reply.nodes = if (is.list(replies)) replies$data$children else NULL\n    return(list(Attribute, lapply(reply.nodes,function(x){GetAttribute(x,feature)})))  \n  }\n  \n  get.structure = function(node, depth=0) {\n    if(is.null(node)) {return(list())}\n    filter     = is.null(node$data$author)\n    replies     = node$data$replies\n    reply.nodes = if (is.list(replies)) replies$data$children else NULL\n    return(list(paste0(filter,\" \",depth), lapply(1:length(reply.nodes), function(x) get.structure(reply.nodes[[x]], paste0(depth, \"_\", x)))))\n  }\n  \n  # setting up the data frame\n  data_extract = data.frame(id               = numeric(),\n                            structure        = character(),\n                            post_date        = as.Date(character()),\n                            comm_date        = as.Date(character()),\n                            num_comments     = numeric(),\n                            subreddit        = character(),\n                            upvote_prop      = numeric(),\n                            post_score       = numeric(),\n                            author           = character(),\n                            user             = character(),\n                            comment_score    = numeric(),\n                            controversiality = numeric(),\n                            comment          = character(),\n                            title            = character(),\n                            post_text        = character(),\n                            link             = character(),\n                            domain           = character(),\n                            URL              = character())\n  \n  \n  pb = utils::txtProgressBar(min = 0, max = length(URL), style = 3)\n  \n  for(i in seq(URL)){\n    \n    if(!grepl(\"^https?://(.*)\",URL[i])) URL[i] = paste0(\"https://www.\",gsub(\"^.*(reddit\\\\..*$)\",\"\\\\1\",URL[i]))\n    if(!grepl(\"\\\\?ref=search_posts$\",URL[i])) URL[i] = paste0(gsub(\"/$\",\"\",URL[i]),\"/?ref=search_posts\")\n    \n    X        = paste0(gsub(\"\\\\?ref=search_posts$\",\"\",URL[i]),\".json?limit=500\") # 500 is the maximum\n    raw_data = tryCatch(RJSONIO::fromJSON(readLines(X, warn = FALSE)),error = function(e) NULL)\n    \n    # try again if it fails\n    if(is.null(raw_data)){\n      Sys.sleep(min(1,wait_time))\n      raw_data = tryCatch(RJSONIO::fromJSON(readLines(X, warn = FALSE)),error = function(e) NULL)\n    }\n    \n    if(is.null(raw_data)==FALSE){\n      \n      # extracting comment specific information:\n      meta.node     = raw_data[[1]]$data$children[[1]]$data\n      main.node     = raw_data[[2]]$data$children\n      \n      if(min(length(meta.node),length(main.node))>0){\n        \n        structure     = unlist(lapply(1:length(main.node), function(x) get.structure(main.node[[x]], x)))\n        \n        TEMP          =          data.frame(id               = NA,\n                                            structure        = gsub(\"FALSE \",\"\",structure[!grepl(\"TRUE\",structure)]),\n                                            post_date        = format(as.Date(as.POSIXct(meta.node$created_utc,origin=\"1970-01-01\")),\"%d-%m-%y\"),\n                                            comm_date        = format(as.Date(as.POSIXct(unlist(lapply(main.node, function(x){GetAttribute(x,\"created_utc\")})),\n                                                                                         origin=\"1970-01-01\")),\"%d-%m-%y\"),\n                                            num_comments     = meta.node$num_comments,\n                                            subreddit        = ifelse(is.null(meta.node$subreddit),\"UNKNOWN\",meta.node$subreddit),\n                                            upvote_prop      = meta.node$upvote_ratio,\n                                            post_score       = meta.node$score,\n                                            author           = meta.node$author,\n                                            user             = unlist(lapply(main.node, function(x){GetAttribute(x,\"author\")})),\n                                            comment_score    = unlist(lapply(main.node, function(x){GetAttribute(x,\"score\")})),\n                                            controversiality = unlist(lapply(main.node, function(x){GetAttribute(x,\"controversiality\")})),\n                                            comment          = unlist(lapply(main.node, function(x){GetAttribute(x,\"body\")})),\n                                            title            = meta.node$title,\n                                            post_text        = meta.node$selftext,\n                                            link             = meta.node$url,\n                                            domain           = meta.node$domain,\n                                            URL              = URL[i],\n                                            stringsAsFactors = FALSE)\n        \n        TEMP$id = 1:nrow(TEMP)\n        \n        if(dim(TEMP)[1]>0 & dim(TEMP)[2]>0) data_extract = rbind(TEMP,data_extract)\n        else print(paste(\"missed\",i,\":\",URL[i]))\n        \n      }\n      \n    }\n    \n    utils::setTxtProgressBar(pb, i)\n    \n    Sys.sleep(min(2,wait_time))\n  }\n  \n  close(pb)\n  \n  return(data_extract)\n  \n}\n\n#' Get all data attributes from search query\n#' \n#' @examples\n#' \\dontrun{\n#' reddit_data = get_reddit(search_terms = \"science\",subreddit = \"science\",cn_threshold=10)\n#' }\n#' \n#' @param search_terms A string of terms to be searched on Reddit.\n#' @param regex_filter An optional regular expression filter that will remove URLs with titles that do not match the condition.\n#' @param subreddit An optional character string that will restrict the search to the specified subreddit.\n#' @param cn_threshold Comment number threshold that remove URLs with fewer comments that cn_threshold. 0 by default.\n#' @param page_threshold Page threshold that controls the number of pages is going to be searched for a given search word. 1 by default.\n#' @param sort_by Sorting parameter, either \"comments\" (default) or \"new\".\n#' @param wait_time wait time in seconds between page requests. 2 by default and it is also the minimum (API rate limit).\n#' @return A data frame with structure / position of the comment with respect to other comments (structure), ID (id), post / thread date (post_date), \n#' comment date (comm_date), number of comments within a post / thread (num_comments), subreddit (subreddit)\n#' upvote proportion (upvote_prop), post /thread score (post_score), author of the post / thread (author), user corresponding to the comment (user),\n#' comment score (comment_score), controversiality (controversiality), comment (comment), title (title), post / thread text (post_text), URL referenced (link)\n#' domain of the references URL (domain)\n#' @export\n\nget_reddit = function(search_terms=NA,\n                      regex_filter=\"\",\n                      subreddit=NA,\n                      cn_threshold=1,\n                      page_threshold=1,\n                      sort_by=\"comments\",\n                      wait_time=2){\n  \n  URL = unique(as.character(reddit_urls(search_terms,\n                                        regex_filter,\n                                        subreddit,\n                                        cn_threshold,\n                                        page_threshold,\n                                        sort_by,\n                                        wait_time)$URL))\n  \n  retrieved_data = reddit_content(URL,wait_time)\n  \n  return(retrieved_data)\n  \n}\n\n#' Create a graph file from a single Reddit thread\n#' \n#' @examples\n#' \\dontrun{\n#' my_url = \"reddit.com/r/web_design/comments/2wjswo/design_last_reordering_the_web_design_process/\"\n#' url_data = reddit_content(my_url)\n#' graph_object = construct_graph(url_data)\n#' }\n#' \n#' @param content_data A data frame produced by reddit_content command\n#' @param plot A logical parameter indicating whether a graph is to be plotted or no (TRUE by default). Note that the root node corresponds to the thread itself rather than any of the comments.\n#' @param write_to A character string specifying the path and file name where a graph object will be saved. The file must end with an extension such as *.gml. \n#' The following formats are allowed: edgelist, pajek, ncol,lgl, graphml, dimacs, gml, dot\", leda.\n#' @return A graph object\n#' @export\n\n\nconstruct_graph = function(content_data,plot=TRUE,write_to=NA){\n  \n  edges_frame = content_data[,c(\"id\",\"structure\")]\n  if(sum(duplicated(edges_frame$structure))>0){\n    stop(\"duplicates found in the structure variable -- make sure that content_data only contains a single thread\")\n  }\n  \n  edges_frame$structure = paste0(\"0_\",as.character(edges_frame$structure))\n  edges_frame = rbind(data.frame(id=\"0\",structure=\"0\",stringsAsFactors=F),edges_frame)\n  \n  \n  A = B = edges_frame\n  colnames(A) = c(\"from_id\",\"from_structure\")\n  colnames(B) = c(\"to_id\",  \"to_structure\")\n  edges_frame = merge(A,B)\n  edges_frame = edges_frame[sapply(1:nrow(edges_frame), \n                                   function(x)grepl(paste0(\"^\",edges_frame$from_structure[x],\"_\\\\d+$\"),\n                                                    edges_frame$to_structure[x])), c(\"from_id\",\"to_id\")]\n  \n  nodes_frame = content_data\n  extra_row   = data.frame(id = 0, comment_score = 0, author=nodes_frame$author[1], controversiality = 0, stringsAsFactors=T)\n  other_cols  = setdiff(colnames(nodes_frame),colnames(extra_row))\n  cols_to_add = as.data.frame(matrix(NA,nrow=1,ncol=length(other_cols)))\n  colnames(cols_to_add) = other_cols\n  extra_row = cbind(extra_row,cols_to_add)\n  nodes_frame = rbind(extra_row,nodes_frame)\n  nodes_frame$start_node = ifelse(nodes_frame$id==0,1,0)\n  nodes_frame = nodes_frame[,-which(colnames(nodes_frame)==\"comment\")] \n  # comments create problems when you export the file and try to read it with other software\n  \n  if(!is.na(write_to)){\n    \n    fmt = gsub(\".*\\\\.(\\\\w+)$\",\"\\\\1\",write_to)\n    \n    if(!(fmt %in% c(\"edgelist\", \"pajek\", \"ncol\",\"lgl\", \"graphml\", \"dimacs\", \"gml\", \"dot\", \"leda\"))){\n      stop(\"export_format must be either edgelist, pajek, ncol, lgl, graphml, dimacs, gml, dot, or leda. See igraph::write.graph for details.\")\n    }\n    igraph::write.graph(igraph::graph.data.frame(edges_frame, directed = T, vertices = nodes_frame),write_to,format = fmt)\n  }\n  \n  if(plot==T){\n    \n    \n    colscale = function(x){\n      if(min(x$comment_score)<0){\n        min_col  = \"firebrick2\"\n        mincolsq = seq(min(as.numeric(x$comment_score)), 0, length.out = ceiling(nrow(x)/2))\n        maxcolsq = seq(0, max(as.numeric(x$comment_score)), length.out = ceiling(nrow(x)/2))\n        if(nrow(x) %% 2 == 1){\n          colsq  = c(mincolsq[-length(mincolsq)],maxcolsq[-1])\n        } else{\n          colsq  = c(mincolsq,maxcolsq)\n        }\n      } else{\n        min_col = \"ivory\"\n        colsq   = seq(0,max(as.numeric(x$comment_score)),length.out = nrow(x))\n      }\n      RP  = grDevices::colorRampPalette(c(min_col,\"royalblue\"))\n      col = RP(nrow(x))[c(findInterval(x$comment_score,colsq,all.inside=T))]\n      return(col)\n    }\n    \n    g = igraph::graph.data.frame(edges_frame, directed = T, vertices = nodes_frame)\n    plot(g,\n         vertex.frame.color = ifelse(igraph::V(g)$controversiality==1,\"red\",NA),\n         layout             = igraph::layout.reingold.tilford,\n         vertex.label       = ifelse(igraph::V(g)$start_node==1,igraph::V(g)$author,igraph::V(g)$user),\n         vertex.label.cex   = ifelse(igraph::V(g)$start_node==1,1,0.65),\n         vertex.label.color = ifelse(igraph::V(g)$controversiality==1,\"darkred\",\"navy\"),\n         vertex.label.font  = ifelse(igraph::V(g)$author==igraph::V(g)$user,4,1),\n         vertex.size        = ifelse(igraph::V(g)$start_node==1,20,12),\n         vertex.color       = ifelse(igraph::V(g)$start_node==1,\"orange2\",colscale(nodes_frame)),\n         edge.arrow.size    = 0.5,\n         edge.color         = \"skyblue2\")\n    graphics::title(content_data$title[1],cex.main=0.75,col.main=\"royalblue\")\n  }\n  \n  return(igraph::graph.data.frame(edges_frame, directed = T, vertices = nodes_frame))\n  \n}\n\n",
    "created" : 1448071292801.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3085223880",
    "id" : "D156D349",
    "lastKnownWriteTime" : 1448071540,
    "path" : "~/Documents/University/Masters Research/R/Reddit Extractor/RedditExtractoR/R/RedditExtractoR.R",
    "project_path" : "R/RedditExtractoR.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}